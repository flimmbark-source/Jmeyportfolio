{
  "order": 3,
  "slug": "ai-archiving",
  "title": "AI Archiving Assistant",
  "tagline": "Productivity ¬∑ AI Tool",
  "summary": "Returned five researcher hours weekly by deploying a confidence-based tagging assistant that pre-labels archives and routes reviews intelligently.",
  "highlights": [
    "AI UX",
    "Information Architecture",
    "Explainability",
    "Product Discovery"
  ],
  "image": {
    "src": "/images/ai-archiver.png",
    "alt": "AI archiving assistant dashboard"
  },
  "links": [
    {
      "href": "/ai-archiving",
      "label": "View case study",
      "variant": "primary"
    }
  ],
  "caseStudy": {
    "hero": {
      "eyebrow": "Case Study",
      "title": "AI Archiving Assistant",
      "subtitle": "Giving researchers a faster path to institutional knowledge.",
      "summary": "I led discovery and design for a tagging assistant that pairs machine learning predictions with curator controls to reclaim research time.",
      "meta": [
        { "label": "Role", "value": "Product Design Lead" },
        { "label": "Timeline", "value": "10 weeks" },
        { "label": "Team", "value": "Research Lead, ML Engineer, PM" },
        { "label": "Key metric", "value": "5 hrs saved / researcher weekly" }
      ],
      "actions": [
        {
          "label": "Back to portfolio",
          "href": "/",
          "variant": "secondary"
        }
      ]
    },
    "overview": {
      "eyebrow": "Project Overview",
      "title": "Turning backlog chaos into searchable insight",
      "description": "The archives team managed 60k+ assets with inconsistent metadata, slowing discovery and recreating prior work. We explored how AI-generated tags could speed intake without risking provenance.",
      "highlights": [
        "Shadowed researchers to capture labeling heuristics and edge cases.",
        "Mapped scattered taxonomies into a flexible ontology for training.",
        "Prototyped review queues that paired automation with curator control."
      ],
      "media": null
    },
    "challenge": {
      "title": "Researchers were drowning in manual tagging",
      "description": "Processing archival drops meant triaging thousands of assets by hand with inconsistent metadata. Teams rebuilt prior work and overlooked relevant artifacts during grant proposals.",
      "metrics": [
        { "label": "Assets ingested yearly", "value": "60k", "description": "Images, video, and oral histories" },
        { "label": "Average tagging time", "value": "12 min", "description": "Per asset across teams" },
        { "label": "Duplicate effort", "value": "28%", "description": "Re-finding previously cataloged work" }
      ],
      "list": [
        "Fragmented taxonomies across departments produced conflicting labels.",
        "Researchers distrusted black-box automation without quality cues.",
        "Leadership wanted defensible metrics before funding an ML pipeline."
      ]
    },
    "solution": {
      "title": "Confidence-based automation with human guardrails",
      "description": "We introduced a tagging assistant that pre-populates metadata, routes items by confidence, and learns through curator feedback loops. Confidence tiers and human controls kept quality visible at every step.",
      "media": null,
      "steps": [
        {
          "title": "Model training hub",
          "description": "Annotated a gold-standard dataset and surfaced prediction rationales for curator audits."
        },
        {
          "title": "Confidence queues",
          "description": "Segmented intake into auto-approve, needs-review, and escalation lanes with contextual guidance."
        },
        {
          "title": "Feedback instrumentation",
          "description": "Embedded accept or revise actions that retrained the model nightly and surfaced precision metrics."
        }
      ],
      "modules": [
        {
          "kicker": "Suggested tags",
          "title": "Transparent predictions",
          "description": "Each AI tag reveals related assets and training examples so curators trust provenance."
        },
        {
          "kicker": "Bulk actions",
          "title": "Curate at scale",
          "description": "Batch approvals, edits, or taxonomy changes without leaving the review flow."
        },
        {
          "kicker": "Escalation notes",
          "title": "Close the loop",
          "description": "Route nuanced artifacts to subject experts with structured context and due dates."
        }
      ]
    },
    "impact": {
      "title": "Impact: five hours saved per researcher each week",
      "description": "Pilot teams slashed manual tagging while increasing trust in metadata quality. The assistant returned time for analysis and grant prep.",
      "metrics": [
        { "label": "Time saved weekly", "value": "5 hrs" },
        { "label": "Tag accuracy", "value": "92%", "description": "Post-review precision" },
        { "label": "Adoption", "value": "3 teams", "description": "Rolled out across research pods" }
      ],
      "results": [
        "5 hrs saved weekly per researcher fueled deeper grant narratives and outreach.",
        "92% precision cleared leadership‚Äôs governance bar for ML-assisted metadata.",
        "3 research pods adopted the workflow, informing the phased ontology roadmap."
      ]
    },
    "interaction": {
      "title": "Interaction patterns that kept humans in control",
      "description": "We paired AI affordances with explainability cues so archivists always knew why a suggestion appeared and how to adjust it. The cues grounded automation in evidence they could challenge.",
      "cards": [
        {
          "icon": "üß†",
          "title": "Explainability chips",
          "description": "Hover states reveal the model features that triggered each suggestion."
        },
        {
          "icon": "‚öñÔ∏è",
          "title": "Review balance",
          "description": "Queues surface work by effort, keeping heavy lifts evenly distributed."
        },
        {
          "icon": "üìà",
          "title": "Performance snapshots",
          "description": "Inline charts show precision / recall so teams know when to recalibrate."
        }
      ]
    },
    "process": {
      "title": "Process: discovery to pilot in three tracks",
      "description": "We ran parallel sprints on researcher needs, data readiness, and model evaluation to validate the concept quickly. That structure kept insights, data, and infrastructure decisions in lockstep.",
      "sections": [
        {
          "title": "1. Map current-state workflows",
          "description": "Documented intake rituals, tooling, and exceptions through targeted contextual inquiry.",
          "findings": [
            {
              "title": "Hidden taxonomies",
              "description": "Teams kept personal tag spreadsheets that never reached the CMS."
            },
            {
              "title": "Quality anxiety",
              "description": "Curators feared automation might overwrite provenance notes without review."
            }
          ]
        },
        {
          "title": "2. Prototype the assistant",
          "description": "Designed mid-fidelity flows in Figma and paired them with a lightweight inference service for live trials.",
          "steps": [
            { "icon": "üìù", "title": "Scenario scripts", "description": "Created tagging stories across asset types to stress-test UI states." },
            { "icon": "üß™", "title": "Usability loops", "description": "Ran weekly sessions with archivists to tune thresholds and messaging." },
            { "icon": "ü§ñ", "title": "Model tuning", "description": "Partnered with the ML engineer to turn qualitative feedback into feature weights." }
          ]
        },
        {
          "title": "3. Launch and measure",
          "description": "Established success metrics and governance rituals ahead of implementation handoff.",
          "comparison": [
            { "before": "Tagging each asset by hand.", "after": "Auto-suggested tags with one-click approvals." },
            { "before": "Guessing which items to prioritize.", "after": "Confidence queues surfacing at-risk assets." },
            { "before": "No feedback loop for model errors.", "after": "Embedded controls that retrain nightly." }
          ]
        }
      ]
    },
    "conclusion": {
      "title": "Reflection",
      "description": "The assistant showed that explainable AI can augment archivists without erasing their craft. Next we are extending to audio transcripts and tightening governance with legal compliance.",
      "tags": [
        "AI-Assisted Workflows",
        "Knowledge Management",
        "Explainability",
        "Product Discovery"
      ]
    }
  }
}
